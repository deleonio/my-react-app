# ğŸ“š Markdown RAG System

Ein lokales RAG (Retrieval-Augmented Generation) System fÃ¼r Markdown-Dateien, das **Ollama** fÃ¼r Embeddings und **LanceDB** als Vector Database verwendet.

## ğŸš€ Features

- âœ… **Komplett lokal** - keine Cloud-APIs, keine Kosten
- âœ… **Ollama Embeddings** - nutzt `mxbai-embed-large` Modell
- âœ… **LanceDB** - schnelle, persistente Vector Database
- âœ… **Markdown-Parser** - verarbeitet `.md` Dateien rekursiv
- âœ… **Frontmatter-Extraktion** - YAML-Metadaten werden berÃ¼cksichtigt
- âœ… **Smart Chunking** - intelligente Textaufteilung mit Overlap

## ğŸ“‹ Voraussetzungen

1. **Node.js 18+**
2. **Ollama** installiert und gestartet
3. **Embedding-Modell** heruntergeladen

### Ollama Setup

```bash
# Ollama installieren (macOS)
brew install ollama

# Ollama starten
ollama serve

# Embedding-Modell herunterladen
ollama pull mxbai-embed-large
```

## ğŸ”§ Installation

```bash
cd rag
npm install
```

## ğŸ“– Verwendung

### 1. Wissensbasis erstellen (Indexierung)

Indexiert alle Markdown-Dateien aus den `issues/` und `reports/` Ordnern:

```bash
npm run index
```

**Ausgabe:**

```text
ğŸ“ Verzeichnis: /path/to/issues
   42 Markdown-Dateien gefunden

   ğŸ“„ issue-1.md... âœ“ (3 chunks)
   ğŸ“„ issue-2.md... âœ“ (2 chunks)
   ...

âœ… Indexierung abgeschlossen: 156 Chunks
```

### 2. Suche durchfÃ¼hren

```bash
npm run search "Button Komponente Problem"
npm run search "Accessibility ARIA"
npm run search "Performance Optimierung"
```

**Ausgabe:**

```text
ğŸ” Suche: "Button Komponente Problem"

  1. ğŸ¯ Relevanz: 87.3%
     ğŸ“‚ issues/issue-42.md
     ğŸ“‘ Chunk 1/3

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚ ## Button-Komponente zeigt Fehler
     â”‚
     â”‚ Die Button-Komponente wirft einen Fehler...
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### 3. Interaktive Suche

Startet eine interaktive Shell fÃ¼r mehrere Suchanfragen:

```bash
npm run interactive
```

**Befehle in der interaktiven Shell:**

- `<text>` - Sucht nach relevantem Inhalt
- `stats` - Zeigt Statistiken
- `help` - Zeigt Hilfe
- `exit` - Beendet das Programm

### 4. Statistiken anzeigen

```bash
npm run stats
```

## ğŸ—ï¸ Projektstruktur

```text
rag/
â”œâ”€â”€ package.json        # Projekt-Konfiguration
â”œâ”€â”€ README.md           # Diese Dokumentation
â”œâ”€â”€ lancedb/            # Vector Database (wird erstellt)
â””â”€â”€ src/
    â”œâ”€â”€ rag.js          # Haupt-RAG-Klasse
    â”œâ”€â”€ index.js        # Indexierungs-Script
    â”œâ”€â”€ search.js       # Such-Script
    â”œâ”€â”€ interactive.js  # Interaktive Shell
    â””â”€â”€ stats.js        # Statistik-Script
```

## âš™ï¸ Konfiguration

Die RAG-Klasse kann mit verschiedenen Optionen konfiguriert werden:

```javascript
const rag = new OllamaLanceRAG({
	dbPath: './lancedb', // Pfad zur LanceDB
	model: 'mxbai-embed-large', // Ollama Embedding-Modell
	tableName: 'kolibri_docs', // Name der Tabelle
	chunkSize: 1000, // Max. Chunk-GrÃ¶ÃŸe in Zeichen
	chunkOverlap: 100, // Overlap zwischen Chunks
	ollamaUrl: 'http://localhost:11434', // Ollama API URL
});
```

### Alternative Embedding-Modelle

```bash
# Andere verfÃ¼gbare Modelle
ollama pull mxbai-embed-large      # GrÃ¶ÃŸer, prÃ¤ziser
ollama pull snowflake-arctic-embed # Gut fÃ¼r mehrsprachig
ollama pull all-minilm             # Kleiner, schneller
```

## ğŸ” Programmatische Verwendung

```javascript
import { OllamaLanceRAG } from './src/rag.js';

async function example() {
	const rag = new OllamaLanceRAG();

	// Initialisieren
	await rag.initialize();

	// Verzeichnisse indexieren
	await rag.indexDirectories(['./issues', './reports']);

	// Suchen
	const results = await rag.search('Button Problem', 5);

	results.forEach((result) => {
		console.log(`${result.similarity * 100}% - ${result.fileName}`);
		console.log(result.text);
	});

	// Statistiken
	const stats = await rag.getStats();
	console.log(stats);
}
```

## ğŸ› Fehlerbehebung

### Ollama nicht erreichbar

```bash
# PrÃ¼fen ob Ollama lÃ¤uft
curl http://localhost:11434/api/tags

# Ollama starten
ollama serve
```

### Modell nicht gefunden

```bash
# VerfÃ¼gbare Modelle anzeigen
ollama list

# Modell herunterladen
ollama pull mxbai-embed-large
```

### Keine Ergebnisse bei der Suche

- FÃ¼hre zuerst `npm run index` aus
- PrÃ¼fe ob Markdown-Dateien in `issues/` oder `reports/` existieren
- Nutze `npm run stats` um die Wissensbasis zu Ã¼berprÃ¼fen

## ğŸ“Š Wie funktioniert es?

1. **Einlesen**: Markdown-Dateien werden rekursiv gelesen
2. **Chunking**: Text wird in Ã¼berlappende Abschnitte geteilt
3. **Embedding**: Ollama erstellt Vektoren fÃ¼r jeden Chunk
4. **Speichern**: Vektoren werden in LanceDB persistiert
5. **Suche**: Query wird vektorisiert und Ã¤hnliche Chunks gefunden
6. **Ranking**: Ergebnisse nach Cosinus-Ã„hnlichkeit sortiert

## ğŸ“ Lizenz

MIT
